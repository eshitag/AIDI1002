{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title: AIDI 1002 Final Term Project Report\n\n#### Nishank Bhola : 200505481@student.georgianc.on.ca\n#### Eshita Gupta : 200503733@student.georgianc.on.ca\n\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction:\n\n#### Problem Description:\n\nBERT is a language model designed to understand language like humans do. One of the biggest challenges in natural language processing (NLP) is understanding the context and meaning of words and phrases within a sentence or document. BERT helps solve this problem by using a bidirectional approach to text processing that can understand the context of a word or phrase based on the surrounding text. This makes it very effective in tasks such as speech comprehension, answering questions, and analyzing emotions. A powerful tool to move the field forward. \n\n#### Context of the Problem:\n\nThis is important because it reduces the need for large amounts of labeled data that are expensive and time consuming to collect. The success of BERT has also spurred further research to develop new and improved language representation models that further advance the field of NLP.\n\nOverall, BERT's contribution to his NLP has been significant, enabling researchers and practitioners to develop more efficient and effective NLP models. \n\n#### Limitation About other Approaches:\n\nPrior approaches to BERT, such as ELMo and GPT, were limited in their ability to capture contextual information from both left and right sides of a text sequence. These models were trained in a unidirectional manner, which meant they were only able to consider the preceding context of a given word. As a result, they were not as effective at capturing the full meaning of a text sequence and required more training data to achieve similar levels of performance as BERT.\n\n#### Solution:\n\nBERT overcomes the limitations of prior approaches by pretraining deep bidirectional representations from unlabeled text, allowing it to consider both left and right context in all layers. This makes it more effective at capturing the full meaning of a text sequence, even in cases where the meaning may not be immediately apparent from the preceding context.","metadata":{}},{"cell_type":"markdown","source":"# Background\n\nExplain the related work using the following table\n\n| Reference |Explanation |  Dataset/Input |Weakness\n| --- | --- | --- | --- |\n| Alan Akbik, Duncan Blythe, and Roland Vollgraf. [1] | The paper introduces a new sequence labeling model called \"Contextual String Embeddings\" (CSE).| CoNLL-2003 (English), GermEval 2014 (German), and WikiNER (English). | The weakness of the paper is that the datasets used for evaluation are primarily in English, so it is unclear how well CSE would perform on non-English languages.\n| Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones[2] | The paper proposes a new language model architecture called \"Deep Levenshtein Transformer\" (DLT) that utilizes deep self-attention and character-level embeddings to improve language modeling at the character level.| Penn Treebank, WikiText-2, and text8. |  The evaluation is primarily focused on language modeling tasks, and it is unclear how well DLT would perform on other natural language processing tasks such as text classification or sequence labeling.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Improvisation\nFine-tuning: BERT can be fine-tuned on a specific NLP task by training it on a task-specific dataset. This can improve the model's performance on that task.","metadata":{}},{"cell_type":"markdown","source":"# Methodology\nWe are going to run BERT normally and print the results and after that we will fine-tune the bert model using our code and improvise it.\n\nWhich will incrase BERT's performance on binary classification tasks.\nwe will use the following steps to achieve it\n\n1. Data Collection: The first step is to collect the dataset. In this code, we are using the fetch_20newsgroups function from the sklearn.datasets module to download the 20 Newsgroups dataset.\n\n2. Data Preprocessing: Once we have the dataset, we need to preprocess it to make it suitable for training our model. In this code, we are using the train_test_split function from the sklearn.model_selection module to split the dataset into training and testing sets. We are also using the BertTokenizer class from the transformers module to tokenize the input text.\n\n3. Model Creation: After preprocessing the data, we need to create the model. In this code, we are using the BertForSequenceClassification class from the transformers module to create a BERT model for sequence classification. We are also setting the number of output labels to the number of unique labels in our dataset.\n\n4. Model Training: Once the model is created, we can train it on the preprocessed data. In this code, we are using the AdamW optimizer from the transformers module to optimize our model's parameters. We are also defining the number of epochs and the batch size for training.\n\n5. Evaluation: After training the model, we need to evaluate its performance on a test set. In this code, we are using a PyTorch DataLoader object to load the test set in batches. We are also computing the accuracy of the model on the test set.\n\n6. Model Deployment: Finally, after evaluating the model's performance, we can deploy it for making predictions on new data.\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Implementation","metadata":{}},{"cell_type":"markdown","source":"BERT is a complex model with millions of parameters, and creating an equivalent model from scratch would require a significant amount of expertise and computational resources.\n\nThat being said, there are several open-source libraries available in Python that provide pre-trained BERT models, such as the Transformers library from Hugging Face. This library allows you to easily use pre-trained BERT models and fine-tune them for your specific NLP task.\n\nSo, we are implementing BERT_UNCASED using hugging face","metadata":{}},{"cell_type":"code","source":"!pip install transformers\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='bert-base-uncased')\nunmasker(\"I'm a [MASK] scientist.\")","metadata":{"execution":{"iopub.status.busy":"2023-04-23T20:07:27.933108Z","iopub.execute_input":"2023-04-23T20:07:27.933393Z","iopub.status.idle":"2023-04-23T20:07:59.075822Z","shell.execute_reply.started":"2023-04-23T20:07:27.933363Z","shell.execute_reply":"2023-04-23T20:07:59.074626Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba1806019d34a0196f917aba1e7b3b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baecd48fa84e4cf98adf5a8da0675788"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"636adc918f2e4cf6ab62068c323afb3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9e300737e37490ca3ba78437d976cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bca3fbd3aa94aab8a72edc42b95a83e"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.32468798756599426,\n  'token': 7596,\n  'token_str': 'rocket',\n  'sequence': \"i'm a rocket scientist.\"},\n {'score': 0.12425611913204193,\n  'token': 5506,\n  'token_str': 'mad',\n  'sequence': \"i'm a mad scientist.\"},\n {'score': 0.11185978353023529,\n  'token': 8235,\n  'token_str': 'brilliant',\n  'sequence': \"i'm a brilliant scientist.\"},\n {'score': 0.08490728586912155,\n  'token': 2307,\n  'token_str': 'great',\n  'sequence': \"i'm a great scientist.\"},\n {'score': 0.05032946541905403,\n  'token': 2470,\n  'token_str': 'research',\n  'sequence': \"i'm a research scientist.\"}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Implementing our code to increase the performance of binary classification","metadata":{}},{"cell_type":"code","source":"# Install the necessary libraries\n!pip install transformers\n!pip install torch\n!pip install scikit-learn\n# Import the required libraries\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset and split it into train and test sets\nnewsgroups = fetch_20newsgroups(subset='all')\ntrain_data, test_data, train_labels, test_labels = train_test_split(newsgroups.data, newsgroups.target, test_size=0.9, random_state=42)\n\n# Tokenize the input text\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntrain_encodings = tokenizer(train_data, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_data, truncation=True, padding=True, max_length=512)\n\n# Convert the labels to tensors\ntrain_labels = torch.tensor(train_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Define the model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(newsgroups.target_names))\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(\"model done\")\n# Set the optimizer and the training parameters\noptimizer = AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 1\nbatch_size = 8\ncount = 0\n# Create a PyTorch DataLoader object for the training and test data\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), train_labels)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), test_labels)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\nprint(\"object created\")\n# Train the model\nfor epoch in range(num_epochs):\n    #print(\"traincount : \"+ str(count+1))\n    # Training\n    model.train()\n    for batch in train_loader:\n        # Load the data to the device\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        # Backward pass\n        loss.backward()\n\n        # Update the parameters\n        optimizer.step()\n    print(\"evaluation started\")\n    # Evaluation\n    model.eval()\n    correct = 0\n    total = 0\n    count = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            #print(\"Evalcount : \"+ str(count+1))\n            # Load the data to the device\n            input_ids = batch[0].to(device)\n            attention_mask = batch[1].to(device)\n            labels = batch[2].to(device)\n\n            # Forward pass\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n\n            # Compute the accuracy\n            total += labels.size(0)\n            correct += (predictions == labels).sum().item()\n\n    accuracy = correct / total\n    print('Epoch:', epoch, 'Accuracy:', accuracy)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T20:07:59.078450Z","iopub.execute_input":"2023-04-23T20:07:59.079207Z","iopub.status.idle":"2023-04-23T20:18:16.576706Z","shell.execute_reply.started":"2023-04-23T20:07:59.079164Z","shell.execute_reply":"2023-04-23T20:18:16.575542Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (1.0.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"model done\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"object created\nevaluation started\nEpoch: 0 Accuracy: 0.6682584600872539\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This code fine-tunes the BERT model for a binary classification task on the provided training data. You can replace the train_data and train_labels variables with your own training data and labels. The code assumes that the labels are binary (0 or 1). If you have multi-class labels, you need to modify the num_labels argument when defining the model, and use a multi-class loss function during training.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion and Future Direction\n\nIn this code, we fine-tuned a pre-trained BERT model for text classification on the 20 Newsgroups dataset. We achieved a test set accuracy of around 66% using a subset of the dataset.\n\nBy fine-tuning BERT on a specific task, we can leverage the model's pre-trained weights to achieve state-of-the-art performance on that task with minimal training data. This approach is particularly useful when we have limited labeled data and want to quickly build an accurate model for a specific task.\n\nThe further direction would be to explore other text classification datasets and fine-tune BERT on them. We can also try different hyperparameters, such as learning rate, batch size, and number of epochs, to further improve the model's performance. Additionally, we can fine-tune other pre-trained language models, such as GPT-2 and RoBERTa, for text classification tasks.","metadata":{}},{"cell_type":"markdown","source":"# References:\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638–1649.\n\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\n\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817–1853.\n\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe fifth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}